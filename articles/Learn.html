<!doctype html>
<html lang="en">
    <head>
        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width,initial-scale=1" />
        <title>How We Learn Step-Level Rewards — Online Process Reward Learning
            (OPRL) — Akhilesh Pant</title>

        <!-- Google Font -->
        <link
            href="https://fonts.googleapis.com/css2?family=Poppins:wght@300;400;600;700;800&display=swap"
            rel="stylesheet">

        <meta name="description"
            content="Online Process Reward Learning (OPRL) — converting sparse terminal rewards into dense step-level feedback using trajectory preferences. Article by Akhilesh Pant.">

        <style>
    :root{
      --bg-900:#060607;
      --card:#07080a;
      --glass: rgba(255,255,255,0.03);
      --muted: #bdbdbd;
      --accent: #00eaff;
      --accent-2: #7c5cff;
      --glass-blur: 12px;
      --max-width:1100px;
    }

    *{box-sizing:border-box}
    html,body{height:100%}
    body{
      margin:0;
      background:
        radial-gradient(1200px 600px at 10% 10%, rgba(0,234,255,0.04), transparent 8%),
        radial-gradient(1000px 500px at 90% 85%, rgba(124,92,255,0.03), transparent 8%),
        var(--bg-900);
      color:#e8eef6;
      font-family:'Poppins',system-ui,-apple-system,Segoe UI,Roboto,Arial;
      -webkit-font-smoothing:antialiased;
      -moz-osx-font-smoothing:grayscale;
      line-height:1.7;
      -webkit-tap-highlight-color:transparent;
      padding-bottom:80px;
    }

    a{color:var(--accent); text-decoration:none}
    .container{max-width:var(--max-width);margin:48px auto;padding:28px;position:relative}

    /* HEADER */
    .topbar{
      position:fixed;left:0;right:0;top:12px;margin:0 auto;max-width:var(--max-width);height:56px;
      display:flex;align-items:center;justify-content:space-between;padding:8px 18px;
      background:linear-gradient(180deg, rgba(255,255,255,0.02), rgba(255,255,255,0.01));
      border-radius:12px;border:1px solid rgba(255,255,255,0.04);
      backdrop-filter: blur(6px);
      box-shadow:0 6px 30px rgba(2,6,23,0.6);
      z-index:40;
    }
    .brand{display:flex;align-items:center;gap:12px}
    .logo{
      width:44px;height:44px;border-radius:9px;
      background:linear-gradient(135deg,var(--accent),var(--accent-2));
      display:flex;align-items:center;justify-content:center;font-weight:800;color:#032;box-shadow:0 6px 30px rgba(0,0,0,0.6);
      font-family:monospace;
    }
    .site-title{font-weight:700;color:#eaffff}
    .navlinks{display:flex;gap:14px;align-items:center}
    .navlinks a{font-size:14px;color:var(--muted);padding:8px 12px;border-radius:8px;transition:all .18s}
    .navlinks a:hover{color:#000;background:linear-gradient(90deg,var(--accent),var(--accent-2))}

    /* HERO */
    .hero{
      margin-top:84px;
      border-radius:14px;
      overflow:hidden;
      background:linear-gradient(180deg, rgba(255,255,255,0.02), rgba(255,255,255,0.01));
      border:1px solid rgba(255,255,255,0.03);
      backdrop-filter: blur(var(--glass-blur));
      display:grid;
      grid-template-columns: 1fr 360px;
      gap:20px;padding:22px;
      align-items:center;
    }
    @media (max-width:960px){ .hero{grid-template-columns:1fr; padding:18px} .hero-right{order:2} }

    .hero-left{padding:6px 8px}
    .kicker{font-size:13px;color:var(--muted);display:flex;gap:8px;align-items:center}
    .title{font-size:28px;color:var(--accent);margin:12px 0 8px;font-weight:800;letter-spacing:-0.4px}
    .subtitle{color:var(--muted);margin-bottom:12px}
    .hero-cta{display:flex;gap:12px;margin-top:12px;flex-wrap:wrap}
    .btn{
      display:inline-flex;align-items:center;gap:10px;padding:10px 14px;border-radius:10px;font-weight:700;
      background:linear-gradient(90deg,var(--accent),var(--accent-2));color:#002;box-shadow:0 8px 30px rgba(0,234,255,0.08);
      border: none;cursor:pointer;transition:transform .15s ease;
    }
    .btn.secondary{background:transparent;color:var(--accent);border:1px solid rgba(255,255,255,0.05)}
    .btn:active{transform:translateY(1px)}

    .hero-right{
      background:linear-gradient(180deg, rgba(255,255,255,0.02), rgba(255,255,255,0.0));
      border-radius:12px;padding:14px;border:1px solid rgba(255,255,255,0.03);
      display:flex;flex-direction:column;gap:12px;align-items:stretch;
      min-height:220px;
    }

    .meta-pill{display:flex;gap:10px;align-items:center;color:var(--muted);font-size:13px}
    .cover-img{width:100%;height:180px;border-radius:10px;object-fit:cover;border:1px solid rgba(255,255,255,0.03)}
    .small-stat{display:flex;gap:8px;align-items:center;justify-content:space-between;padding:8px 12px;background:linear-gradient(180deg, rgba(255,255,255,0.01), transparent);border-radius:10px;color:var(--muted);font-size:13px}

    /* LAYOUT */
    .layout{display:grid;grid-template-columns: 1fr 320px;gap:28px;margin-top:24px}
    @media (max-width:1100px){ .layout{grid-template-columns:1fr} }

    article{
      background:linear-gradient(180deg, rgba(255,255,255,0.01), rgba(255,255,255,0.00));
      border-radius:12px;padding:22px;border:1px solid rgba(255,255,255,0.03);
      backdrop-filter: blur(var(--glass-blur));
      color: #dfe9f3;
    }

    article h2{color:var(--accent);font-size:20px;margin-top:18px}
    article h3{color:var(--accent-2);font-size:16px;margin-top:16px}
    article h4{color:#dbe9ff;font-size:15px;margin-top:10px}
    article p{font-size:15px;color:#e6eef8}
    article ol{padding-left:18px;margin-top:6px}
    article pre{background:#071018;padding:12px;border-radius:8px;overflow:auto;color:#d8f8ff}

    /* highlight box */
    .highlight{
      border-radius:10px;padding:14px;margin:16px 0;border-left:4px solid var(--accent);
      background:linear-gradient(180deg, rgba(0,234,255,0.02), rgba(124,92,255,0.01));
      color:#bbf7ff;
    }

    /* SIDEBAR */
    .sidebar{
      position:relative;top:0;display:flex;flex-direction:column;gap:12px;
    }
    .card{
      border-radius:12px;padding:14px;background:linear-gradient(180deg, rgba(255,255,255,0.01), rgba(255,255,255,0.00));
      border:1px solid rgba(255,255,255,0.03);
    }
    .toc a{display:block;color:var(--muted);padding:8px;border-radius:8px;font-size:14px}
    .toc a:hover{color:var(--accent)}
    .stats{display:flex;gap:8px;flex-wrap:wrap}
    .stat{flex:1;min-width:120px;padding:10px;border-radius:10px;background:rgba(255,255,255,0.01);text-align:center}

    /* FOOTER */
    footer.main-footer{
      margin-top:36px;border-radius:12px;padding:28px;border:1px solid rgba(255,255,255,0.03);
      background:linear-gradient(180deg, rgba(255,255,255,0.01), rgba(255,255,255,0.00));
      backdrop-filter: blur(10px);
    }
    footer .footer-grid{display:grid;grid-template-columns:1fr 360px;gap:20px;align-items:start}
    @media (max-width:900px){ footer .footer-grid{grid-template-columns:1fr} }

    .footer-links{display:flex;flex-wrap:wrap;gap:8px;margin-top:12px}
    .chip{padding:8px 12px;border-radius:999px;background:transparent;border:1px solid rgba(255,255,255,0.04);font-weight:700;color:var(--muted)}
    .chip.primary{border-color:transparent;background:linear-gradient(90deg,var(--accent),var(--accent-2));color:#001;text-shadow:none}
    .socials{display:flex;gap:10px;margin-top:12px}

    .copyright{color:var(--muted);font-size:13px;margin-top:12px}

    /* micro animations */
    .fade-up{opacity:0;transform:translateY(8px);transition:all .6s cubic-bezier(.2,.9,.2,1)}
    .in-view{opacity:1;transform:translateY(0)}
    .glow{box-shadow:0 10px 30px rgba(0,234,255,0.06), inset 0 -6px 30px rgba(124,92,255,0.02)}

    /* small screens */
    @media (max-width:600px){
      .title{font-size:20px}
      .navlinks{display:none}
      .container{padding:16px;margin:64px auto}
      .topbar{left:12px;right:12px}
    }

    /* code figure placeholder */
    .figure{
      width:100%;border-radius:10px;padding:14px;background:#071018;border:1px solid rgba(255,255,255,0.02);color:#cfefff;margin-top:12px;overflow:auto;
    }

  </style>
    </head>
    <body>

        <!-- top nav -->
        <div class="topbar" role="banner" aria-label="Site topbar">
            <div class="brand">
                <div class="logo">AP</div>
                <div>
                    <div class="site-title">Akhilesh Pant</div>
                    <div style="font-size:12px;color:var(--muted)">AI · ML ·
                        Research</div>
                </div>
            </div>

            <nav class="navlinks" role="navigation" aria-label="top navigation">
                <a href="#article">Article</a>
                <a href="#footer">Explore</a>
                <a href="https://github.com/Akhilesh2004NSBW"
                    target="_blank">GitHub</a>
                <a href="mailto:apx.codes@gmail.com" target="_blank">Contact</a>
            </nav>
        </div>

        <main class="container" role="main">
            <div class="hero fade-up" id="hero">
                <div class="hero-left">
                    <div class="kicker">Reinforcement Learning • Reward Design •
                        Preference Learning</div>
                    <h1 class="title">Online Process Reward Learning (OPRL) —
                        <span style="font-weight:700">How We Learn Step-Level
                            Rewards from Preferences</span></h1>
                    <div class="subtitle">Converting sparse terminal rewards
                        into dense, step-by-step feedback using trajectory
                        preferences and online process reward models.</div>
                    <div style="display:flex;gap:12px;align-items:center"
                        class="hero-cta">
                        <a class="btn" href="#article" id="read-btn">▶ Read
                            Article</a>
                        <a class="btn secondary"
                            href="https://github.com/Akhilesh2004NSBW"
                            target="_blank" rel="noopener">All Projects</a>
                    </div>

                    <div
                        style="margin-top:16px;display:flex;gap:10px;flex-wrap:wrap">
                        <div
                            style="color:var(--muted);font-size:13px">Published:
                            Dec 2025</div>
                        <div style="color:var(--muted);font-size:13px"
                            id="reading-time">—</div>
                        <div style="color:var(--muted);font-size:13px">Category:
                            Reinforcement Learning • Rewards</div>
                    </div>

                </div>

                <aside class="hero-right">
                    <img class="cover-img" src="../assets/images/learn.jpg"
                        alt="OPRL — Process reward model illustration"
                        onerror="this.style.display='none'">
                    <div class="small-stat">
                        <div style="display:flex;flex-direction:column">
                            <div style="font-weight:800;color:#e8fbff">Core
                                idea</div>
                            <div style="color:var(--muted);font-size:13px">Learn
                                step-level rewards from pairwise trajectory
                                preferences</div>
                        </div>
                        <div
                            style="text-align:right;color:var(--muted);font-size:13px">Use:
                            credit assignment in sparse tasks</div>
                    </div>
                </aside>
            </div>

            <!-- main layout -->
            <div class="layout">
                <article id="article" class="fade-up" tabindex="0"
                    role="article" aria-labelledby="article-title">
                    <p>
                        Many reinforcement learning (RL) tasks are sparse: the
                        environment only returns a meaningful reward at the
                        end of an episode (for example, +10 on reaching a goal).
                        In such settings, assigning credit to the correct
                        intermediate actions is difficult and learning becomes
                        slow or unstable. <strong>Online Process Reward Learning
                            (OPRL)</strong>
                        transforms sparse terminal feedback into dense
                        step-by-step signals by learning a <em>process reward
                            model</em>
                        from preference comparisons over whole trajectories.
                    </p>

                    <h2 id="article-title">Motivation — why sparse rewards
                        hurt</h2>

                    <p>
                        When an agent receives reward only at episode
                        termination, it can be nearly impossible for the agent
                        to
                        determine which actions earlier in the episode
                        contributed to success. This is especially acute in
                        long-horizon
                        environments with obstacles or delayed outcomes: the
                        agent wanders, rarely finds the goal, and gradient-based
                        updates provide little signal. OPRL addresses this by
                        learning an auxiliary reward function that provides
                        meaningful, dense feedback at each time step.
                    </p>

                    <h3>Environment example: the 8×8 maze</h3>
                    <p>
                        A canonical testbed for OPRL is a grid-world maze (8×8)
                        with obstacles. The agent starts at the top-left
                        cell and the goal is the bottom-right cell. The
                        environment emits a reward only on reaching the goal
                        (sparse),
                        and zero otherwise. Without shaping, training a policy
                        in this maze can take many episodes to stumble into
                        successful paths.
                    </p>

                    <div class="highlight">
                        <strong>Key idea:</strong> learn a small reward for
                        every visited state — a process reward — so the agent
                        receives
                        informative feedback during exploration.
                    </div>

                    <h2>Process Reward Model</h2>
                    <p>
                        The Process Reward Model is a neural network that maps
                        states (or state-action pairs) to a scalar "process"
                        reward for that step. It is trained using preference
                        labels derived from trajectory comparisons rather than
                        direct numeric supervision. This enables learning even
                        when numeric step rewards are unavailable.
                    </p>

                    <h3>Collecting trajectories and preferences</h3>
                    <p>
                        The system collects full episode trajectories from the
                        agent's interactions. Each trajectory contains the
                        sequence
                        of states (and optionally actions) visited and the
                        terminal outcome (success/failure or final return).
                        From these trajectories, the system forms pairwise
                        preferences: given two trajectories, the one with higher
                        final
                        return (or success) is labeled as preferred. These
                        pairwise comparisons are the training signal for the
                        reward model.
                    </p>

                    <h3>Preference learning objective</h3>
                    <p>
                        OPRL commonly uses a Bradley–Terry style likelihood for
                        pairwise comparisons. The reward model scores each
                        trajectory
                        by summing predicted step rewards. The probability that
                        trajectory A is preferred to B is modeled as:
                    </p>

                    <div class="figure" role="figure"
                        aria-label="Bradley-Terry style preference model">
                        <pre>
P(A &gt; B) = sigmoid( S(A) - S(B) )
where S(T) = &Sigma; r_{\theta}(s_t)  (sum of process rewards predicted along trajectory T)
</pre>
                    </div>

                    <p>
                        The model is trained to maximize the probability
                        assigned to the empirically preferred trajectory across
                        many
                        sampled pairs. Over time, the reward model learns to
                        assign higher cumulative scores to trajectories that
                        lead
                        to better terminal outcomes.
                    </p>

                    <h2>Shaping the environment reward</h2>
                    <p>
                        Once the process reward model produces sensible per-step
                        rewards, these are <em>added</em> to the environment's
                        native
                        sparse rewards (reward shaping). The shaped reward used
                        to train the policy becomes:
                    </p>

                    <div class="figure" role="figure"
                        aria-label="Reward shaping formula">
                        <pre>
r_shaped(s_t, a_t, s_{t+1}) = r_env(s_t, a_t, s_{t+1}) + alpha * r_process(s_t)
</pre>
                    </div>

                    <p>
                        Here, <code>alpha</code> is a scaling factor controlling
                        the influence of learned process rewards. The shaped
                        signal
                        gives the policy denser gradients and dramatically
                        improves credit assignment.
                    </p>

                    <h2>Policy learning (Actor–Critic)</h2>
                    <p>
                        With shaped rewards in place, the agent's policy network
                        (commonly an Actor–Critic architecture) is trained using
                        standard RL updates (e.g., A2C, PPO, or other policy
                        gradient methods). The actor learns to produce actions
                        that
                        increase cumulative shaped return while the critic
                        estimates the value using the shaped signal.
                    </p>

                    <h3>Why this works</h3>
                    <ol>
                        <li>Preferences impose a global ordering between
                            trajectories — even weak signals (like final return)
                            are enough to derive training pairs.</li>
                        <li>The process reward model reduces sparse supervision
                            into dense, instructive feedback at each time
                            step.</li>
                        <li>Shaped rewards accelerate policy optimization and
                            improve credit assignment without requiring oracle
                            step-level labels.</li>
                    </ol>

                    <h2>Training workflow (practical)</h2>
                    <ol>
                        <li>Run the current policy to collect a batch of full
                            trajectories.</li>
                        <li>Compute terminal returns and construct pairwise
                            preference pairs (A &gt; B when return_A &gt;
                            return_B).</li>
                        <li>Train the process reward model with the
                            Bradley–Terry objective on these pairs.</li>
                        <li>Use the process model to compute per-step process
                            rewards for new rollouts and form shaped
                            rewards.</li>
                        <li>Train the policy (Actor–Critic) using shaped
                            rewards; repeat.</li>
                    </ol>

                    <h3>Implementation notes</h3>
                    <p>
                        Practical implementations often include:
                    </p>
                    <ul>
                        <li><strong>Memory-efficient history handling:</strong>
                            for long trajectories store compact references (e.g.
                            segments in shared storage) rather than duplicating
                            full sequences.</li>
                        <li><strong>Regularization:</strong> L2 or entropy
                            regularization to keep the process reward model
                            stable and prevent degenerate solutions.</li>
                        <li><strong>Alpha scheduling:</strong> slowly increase
                            alpha from 0 to a desired value so the policy first
                            learns under the natural reward distribution.</li>
                        <li><strong>Mixed preference generation:</strong>
                            besides final-return comparisons, include
                            success/failure signals or human preferences if
                            available.</li>
                    </ul>

                    <h2>Empirical results (what to expect)</h2>
                    <p>
                        Papers and experiments report several consistent
                        benefits when using OPRL-style shaping:
                    </p>

                    <ol>
                        <li><strong>Faster learning:</strong> policy achieves
                            competent behavior in fewer episodes than baseline
                            with sparse reward.</li>
                        <li><strong>Improved success rate:</strong> higher
                            probability of reaching the goal during training and
                            evaluation.</li>
                        <li><strong>Stability:</strong> lower variance in
                            returns across seeds when preferences provide
                            reliable ordering information.</li>
                    </ol>

                    <div class="highlight">
                        Visualization typically used: return curves over
                        training episodes, success rates (fraction of episodes
                        reaching goal), reward-model loss curves, and policy
                        loss curves.
                    </div>

                    <h2>Limitations & considerations</h2>
                    <p>
                        While OPRL is powerful, consider:
                    </p>
                    <ul>
                        <li><strong>Preference quality:</strong> if preference
                            labels are noisy or uninformative, the process model
                            will learn poor shaping rewards.</li>
                        <li><strong>Scaling:</strong> comparing many trajectory
                            pairs can be computationally heavy; use smart
                            sampling and mini-batching.</li>
                        <li><strong>Reward hacking risks:</strong> the process
                            model could learn to reward short-term behaviors
                            that correlate with preference labels but do not
                            generalize — regularization and validation are
                            important.</li>
                    </ul>

                    <h2>Conclusion</h2>
                    <p>
                        Online Process Reward Learning converts sparse terminal
                        supervision into dense, per-step feedback using
                        preference learning over trajectories.
                        By training a process reward model online and using its
                        outputs to shape environment rewards, OPRL improves
                        credit assignment,
                        speeds up policy optimization, and stabilizes learning
                        in sparse-reward domains.
                    </p>

                    <h3>Where to go next</h3>
                    <p>
                        Try OPRL on increasingly complex environments:
                        multi-room mazes, continuous control tasks, or
                        environments with partial observability.
                        Experiment with preference sources (automated returns,
                        human labels) and scaling strategies for pairwise
                        comparisons.
                    </p>

                    <p style="margin-top:18px;color:var(--muted)">If you want, I
                        can add example code snippets (PyTorch/TensorFlow) for a
                        minimal OPRL pipeline, training scripts, or
                        visualizations for returns and preference-model loss.
                        Just say which framework you'd prefer.</p>

                </article>

                <!-- sidebar -->
                <aside class="sidebar fade-up" aria-labelledby="sidebar-title">
                    <div class="card">
                        <div
                            style="display:flex;justify-content:space-between;align-items:center">
                            <div>
                                <div
                                    style="font-size:13px;color:var(--muted)">Quick
                                    links</div>
                                <div
                                    style="font-weight:800;color:#e8fbff">Explore</div>
                            </div>
                            <div style="font-size:12px;color:var(--muted)">RL •
                                Reward Design</div>
                        </div>

                        <div class="toc" style="margin-top:12px">
                            <a href="#article">Article</a>
                            <a href="#article-title">Motivation</a>
                            <a href="#article-title">Process Reward Model</a>
                            <a href="#article-title">Training workflow</a>
                            <a href="#footer">Resources</a>
                        </div>
                    </div>

                    <div class="card">
                        <div
                            style="font-weight:800;color:#e8fbff;margin-bottom:8px">Repository</div>
                        <div style="color:var(--muted);font-size:14px">Example
                            code, notebooks, and reproducible experiments</div>
                        <div style="margin-top:12px;display:flex;gap:8px">
                            <a class="chip primary"
                                href="https://github.com/Marktechpost/AI-Tutorial-Codes-Included/blob/main/Reinforcement%20learning%2Foprl_preference_shaped_rl_Marktechpost.ipynb"
                                target="_blank">OPRL Repo</a>
                            <a class="chip"
                                href="https://github.com/Akhilesh2004NSBW"
                                target="_blank">All Projects</a>
                        </div>
                    </div>

                    <div class="card">
                        <div
                            style="font-weight:800;color:#e8fbff;margin-bottom:8px">Metrics
                            (example)</div>
                        <div class="stats">
                            <div class="stat"><div
                                    style="font-weight:800;color:#fff">↑
                                    Faster</div><div
                                    style="color:var(--muted);font-size:13px">Convergence</div></div>
                            <div class="stat"><div
                                    style="font-weight:800;color:#fff">↑
                                    Success</div><div
                                    style="color:var(--muted);font-size:13px">Rate</div></div>
                            <div class="stat"><div
                                    style="font-weight:800;color:#fff">↓
                                    Var</div><div
                                    style="color:var(--muted);font-size:13px">Across
                                    seeds</div></div>
                        </div>
                    </div>
                </aside>
            </div>

            <!-- FOOTER -->
            <footer id="footer" class="main-footer fade-up" role="contentinfo"
                aria-label="Footer">
                <div class="footer-grid">
                    <div>
                        <h3 style="color:var(--accent);margin:0 0 8px">Explore
                            My Work</h3>

                        <p style="color:var(--muted);margin-top:6px">
                            Browse reproducible RL experiments, notebooks, and
                            research notes. From reward learning and synthetic
                            data pipelines to
                            biometric authentication and quantum simulation —
                            full code and write-ups are on GitHub.
                        </p>

                        <p style="color:var(--muted);margin-top:8px">
                            Subscribe for deep technical walkthroughs, code
                            recipes, and experimental model releases.
                        </p>

                        <div class="footer-links" style="margin-top:14px">
                            <a class="chip"
                                href="https://arxiv.org/pdf/2509.19199"
                                target="_blank">Research
                                Paper</a>
                            <a class="chip"
                                href="https://github.com/Akhilesh2004NSBW"
                                target="_blank">Project Repo</a>
                            <a class="chip"
                                href="https://github.com/Akhilesh2004NSBW"
                                target="_blank">All Repos</a>
                            <a class="chip" href="#" target="_blank">Live
                                Demos</a>
                            <a class="chip"
                                href="https://www.linkedin.com/in/akhilesh2004"
                                target="_blank">LinkedIn</a>
                        </div>

                        <div
                            style="margin-top:12px;display:flex;gap:10px;align-items:center;flex-wrap:wrap">
                            <a class="chip"
                                href="mailto:apx.codes@gmail.com">Email</a>
                            <a class="chip" href="tel:+918477802902">+91
                                8477802902</a>
                            <a class="chip" href="https://wa.me/918477802902"
                                target="_blank">WhatsApp</a>
                        </div>

                        <div class="copyright" style="margin-top:16px">
                            © 2025 Akhilesh Pant — AI Engineer • Machine
                            Learning Specialist • Research & Innovation
                        </div>
                    </div>

                    <div
                        style="display:flex;flex-direction:column;gap:12px;align-items:stretch">
                        <div
                            style="display:flex;justify-content:space-between;align-items:center">
                            <div
                                style="font-weight:800;color:#e8fbff">Subscribe</div>
                            <div style="color:var(--muted);font-size:13px">Stay
                                updated</div>
                        </div>

                        <div style="display:flex;gap:8px">
                            <input id="newsletter-email" type="email"
                                placeholder="example@gmail.com"
                                aria-label="newsletter email"
                                style="flex:1;padding:12px;border-radius:10px;border:1px solid rgba(255,255,255,0.04);background:transparent;color:var(--muted)">
                            <button id="subscribe-btn" class="chip primary"
                                style="cursor:pointer">Subscribe</button>
                        </div>

                        <div
                            style="margin-top:8px;color:var(--muted);font-size:13px">Follow</div>
                        <div style="display:flex;gap:10px;margin-top:6px">
                            <!-- GitHub -->
                            <a href="https://github.com/Akhilesh2004NSBW"
                                target="_blank" aria-label="GitHub">
                                <svg width="36" height="36" viewBox="0 0 24 24"
                                    fill="none"
                                    style="filter:drop-shadow(0 6px 18px rgba(0,0,0,0.4))">
                                    <rect width="24" height="24" rx="7"
                                        fill="rgba(255,255,255,0.02)"></rect>
                                    <path fill="currentColor"
                                        d="M12 .5a12 12 0 0 0-3.8 23.4c.6.1.8-.2.8-.6v-2.1c-3.4.7-4-1.6-4-1.6-.5-1.3-1.1-1.6-1.1-1.6-.9-.6.1-.6.1-.6 1 .1 1.6 1 1.6 1 .9 1.6 2.4 1.2 3 .9.1-.7.4-1.2.7-1.5-2.7-.3-5.5-1.3-5.5-5.9 0-1.3.4-2.4 1.1-3.3-.1-.3-.5-1.6.1-3.2 0 0 .9-.3 3.3 1.2a11.5 11.5 0 0 1 6 0c2.4-1.5 3.3-1.2 3.3-1.2.6 1.6.2 2.9.1 3.2.7.9 1.1 2 1.1 3.3 0 4.6-2.8 5.6-5.5 5.9.4.3.8 1 .8 2v3c0 .4.2.7.8.6A12 12 0 0 0 12 .5z"
                                        fill="#cfefff" />
                                </svg>
                            </a>

                            <!-- LinkedIn -->
                            <a href="https://www.linkedin.com/in/akhilesh2004"
                                target="_blank" aria-label="LinkedIn">
                                <svg width="36" height="36" viewBox="0 0 24 24"
                                    fill="none">
                                    <rect width="24" height="24" rx="7"
                                        fill="rgba(255,255,255,0.02)"></rect>
                                    <path
                                        d="M6.5 9h3v9h-3V9zM8 6.75a1.75 1.75 0 110-3.5 1.75 1.75 0 010 3.5zM14 9c2.1 0 3.5 1.1 3.5 3.4V18h-3v-5.2c0-1.2-.4-1.6-1.1-1.6-.6 0-1 .4-1.2.8-.1.2-.1.5-.1.8V18h-3V9h3v1.2c.4-.7 1.4-1.2 2.7-1.2z"
                                        fill="#cfefff" />
                                </svg>
                            </a>

                            <!-- Instagram -->
                            <a
                                href="https://www.instagram.com/national_security_blackwing"
                                target="_blank" aria-label="Instagram">
                                <svg width="36" height="36" viewBox="0 0 24 24"
                                    fill="none">
                                    <rect width="24" height="24" rx="7"
                                        fill="rgba(255,255,255,0.04)"></rect>
                                    <path
                                        d="M12 7.2c-2.6 0-4.8 2.1-4.8 4.8s2.1 4.8 4.8 4.8 4.8-2.1 4.8-4.8S14.6 7.2 12 7.2zm0 7.8c-1.7 0-3-1.3-3-3s1.3-3 3-3 3 1.3 3 3-1.3 3-3 3zm4.9-8.5c0 .6-.5 1.1-1.1 1.1-.6 0-1-.5-1-1.1s.4-1.1 1-1.1c.6-.1 1.1.4 1.1 1.1z"
                                        fill="#cfefff" />
                                    <path
                                        d="M12 3.2c-1.6 0-1.8 0-2.4.1-.6.1-1.1.2-1.6.4-.5.2-.9.5-1.3.9-.4.4-.7.8-.9 1.3-.2.5-.3 1-.4 1.6-.1.6-.1.8-.1 2.4s0 1.8.1 2.4c.1.6.2 1.1.4 1.6.2.5.5.9.9 1.3.4.4.8.7 1.3.9.5.2 1 .3 1.6.4.6.1.8.1 2.4.1s1.8 0 2.4-.1c.6-.1 1.1-.2 1.6-.4.5-.2.9-.5 1.3-.9.4-.4.7-.8.9-1.3.2-.5.3-1 .4-1.6.1-.6.1-.8.1-2.4s0-1.8-.1-2.4c-.1-.6-.2-1.1-.4-1.6-.2-.5-.5-.9-.9-1.3-.4-.4-.8-.7-1.3-.9-.5-.2-1-.3-1.6-.4-.6-.1-.8-.1-2.4-.1z"
                                        stroke="#cfefff" stroke-width="1.2"
                                        fill="none" />
                                </svg>
                            </a>
                        </div>
                    </div>
                </div>
            </footer>

        </main>

        <script>
    // Reading time
    (function(){
      const article = document.getElementById('article');
      const words = article.innerText.trim().split(/\s+/).length;
      const mins = Math.max(1, Math.round(words/220));
      document.getElementById('reading-time').innerText = mins + ' min read';
    })();

    // Reveal on scroll
    (function(){
      const observer = new IntersectionObserver((entries)=>{
        entries.forEach(e=>{
          if(e.isIntersecting) e.target.classList.add('in-view');
        });
      }, {threshold:0.08});
      document.querySelectorAll('.fade-up').forEach(el=>observer.observe(el));
    })();

    // Smooth anchor scroll
    (function(){
      document.querySelectorAll('a[href^="#"]').forEach(a=>{
        a.addEventListener('click', (ev)=>{
          const href = a.getAttribute('href');
          if(href && href.startsWith('#')){
            ev.preventDefault();
            const el = document.querySelector(href);
            if(el) el.scrollIntoView({behavior:'smooth',block:'center'});
            history.replaceState && history.replaceState(null, '', href);
          }
        });
      });
    })();

    // Subscribe button (light demo, non-blocking)
    (function(){
      const btn = document.getElementById('subscribe-btn');
      btn && btn.addEventListener('click', ()=>{
        const e = document.getElementById('newsletter-email');
        const val = e && e.value && e.value.trim();
        if(!val || !/^[^\s@]+@[^\s@]+\.[^\s@]+$/.test(val)){
          e && (e.style.borderColor = '#ff6b6b');
          btn.innerText = 'Enter valid email';
          setTimeout(()=>{ btn.innerText = 'Subscribe'; e && (e.style.borderColor='rgba(255,255,255,0.04)') },1800);
          return;
        }
        // Demo success animation
        btn.innerText = 'Subscribed ✓';
        btn.style.opacity = '0.9';
        setTimeout(()=>{ btn.innerText = 'Subscribe'; e.value=''; },2200);
      });
    })();
  </script>
    </body>
</html>
